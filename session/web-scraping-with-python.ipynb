{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML basics\n",
    "\n",
    "### What _is_ a web page, anyway?\n",
    "\n",
    "Generally, a web page consists of specifically formatted text files. A few common types are:\n",
    "\n",
    "- `.html` (HyperText Markup Language) files give structure to a page\n",
    "- `.css` (Cascading Style Sheet) files determine how the page looks\n",
    "- `.js` (JavaScript) files add interactivity\n",
    "\n",
    "Today, we'll focus on the HTML, which gives structure to the page.\n",
    "\n",
    "Most HTML elements are represented by a pair of tags -- an opening tag and a closing tag.\n",
    "\n",
    "A table, for example, starts with `<table>` and ends with `</table>`. The first tag tells the browser: \"Hey! I got a table here! Render it as a table.\" The closing tag (note the forward slash!) tells the browser: \"Hey! I'm all done with that table, thanks.\" Inside the table are nested more HTML tags representing rows (`<tr>`) and cells (`<td>`).\n",
    "\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\" class=\"striped-table\" style=\"width: 95%\">\n",
    "    <tr> <!-- Header -->\n",
    "        <td>Column A</td>\n",
    "        <td>Column B</td>\n",
    "        <td>Column C</td>\n",
    "    </tr>\n",
    "    <tr> <!-- Row 1 --->\n",
    "        <td>Row 1, Column A</td>\n",
    "        <td>Row 1, Column B</td>\n",
    "        <td>Row 1, Column C</td>\n",
    "    </tr>\n",
    "    <tr> <!-- Row 2 --->\n",
    "        <td>Row 2, Column A</td>\n",
    "        <td>Row 2, Column B</td>\n",
    "        <td>Row 2, Column C</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "HTML elements can have any number of attributes, such as IDs, which uniquely identify elements --\n",
    "\n",
    "```html\n",
    "<table id=\"example-table\">\n",
    "```\n",
    "\n",
    "-- classes, which identify a type of element --\n",
    "\n",
    "```html\n",
    "<table class=\"striped-table\">\n",
    "```\n",
    "\n",
    "-- and styles, which define how specific elements appear --\n",
    "\n",
    "```html\n",
    "<table style=\"width:95%;\">\n",
    "```\n",
    "\n",
    "-- that will be useful to know about when we're scraping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting HTML in your browser\n",
    "\n",
    "You can look at the HTML that makes up a web page by _inspecting the source_ in a web browser. We like Chrome and Firefox for this; today, we'll use Chrome.\n",
    "\n",
    "#### Inspect element\n",
    "\n",
    "You can inspect specific elements on the page by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\" from the context menu that pops up. Hover over elements in the \"Elements\" tab to highlight them on the page. This can be helpful when you're trying to figure how to uniquely identify the element you want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View page source\n",
    "\n",
    "To examine all of the source code that makes up a page, you can \"view source.\" In Chrome, right click anywhere in your browser window, then click \"View Page Source.\"\n",
    "\n",
    "This will open a new tab showing you all of the HTML code that makes up that page. Ignore 99% of it and try to locate the element(s) that you want to target (use `Ctrl+F` on a PC and `⌘+F` to find)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Let's give our new skills a whirl. Open up a Chrome browser and navigate to [the first page of Maryland's list of WARN letters](https://www.dllr.state.md.us/employment/warn.shtml). Your goal is to isolate the table element on the page.\n",
    "\n",
    "There are many ways to grab content from HTML, and every page you scrape data from will require a slightly different trick. At this stage, your job is to find a pattern or identifier in the code for the element you’d like to extract, which we will then give as instructions to our Python code.\n",
    "\n",
    "Inspect the table element or view the page source. How would you pick the table element out from the all of the elements that make up the page? Is it the only table? If not, does it have any attributes, like a class or ID, that would allow you to target it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Python\n",
    "\n",
    "The remainder of this notebook demonstrates how you can use the Python programming language to scrape information from a web page. The goal today: Scrape the main table on [the first page of Maryland's list of WARN letters](https://www.dllr.state.md.us/employment/warn.shtml) and, if time, write the data to a CSV that you can open with Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "As we learned in the first section, Python provides some broadly useful objects, like strings and lists, and functions, like `type()` and `print()`, by default. For more specialized operations, like making web requests and parsing HTML, we need to import a few modules, from which we can access helpful objects and functions.\n",
    "\n",
    "Today, we'll use two third-party Python libraries to help us scrape:\n",
    "- `requests` is the de facto standard for making HTTP requests, similar to what happens when you type a URL into a browser window and hit enter.\n",
    "- `bs4`, or BeautifulSoup, is a popular library for parsing HTML into a data structure that Python can work with.\n",
    "\n",
    "**Note:** These libraries are installed separately from Python on a per-project basis. They're already in your working environment for this tutorial. If you want to revisit this tutorial on your own computer, or if you want to create a scraping project of your own, you can ([read more about IRE's recommendations for setting up Python projects here](https://docs.google.com/document/d/1cYmpfZEZ8r-09Q6Go917cKVcQk_d0P61gm0q8DAdIdg/edit#heading=h.od2v1nkge5t1))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `import` keyword to import the `requests` and `bs4` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, you can use the `dir()` function to inspect the objects and methods included with a module, and the `help()` function to read more about a particular object or method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request the page\n",
    "\n",
    "Next, we'll use the `get()` method of the `requests` library (which we just imported) to grab the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `help()` function to learn more about `requests.get`.\n",
    "\n",
    "**Note**: Some third-party libraries come with more documentation than others, however popular ones (such as `requests`) tend to be just as, if not more, helpful than the standard Python library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(requests.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define a variable `URL` as a string containing the web address we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://www.dllr.state.md.us/employment/warn.shtml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use `requests.get` to retrieve the URL. Remember that you can can assign the output of an expression to a variable, so we'll store the response as a new variable. The variable name is arbitrary, but it's a great idea to use something that describes the value it's pointing to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warn_page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to make sure that your request was successful, you can check the `status_code` attribute of the Python object that was returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warn_page.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `200` code means all is well. `404` means the page wasn't found, etc. [Here's one of our favorite lists of HTTP status codes](https://http.cat/) ([or here, if you prefer dogs](https://httpstatusdogs.com/)).\n",
    "\n",
    "The object being stored as the `warn_page` variable came back with a lot of potentially useful information we could access. Use the `dir()` method to see all the attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(warn_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we're mostly interested in the `.text` attribute -- the HTML that makes up the web page, same as if we'd viewed the page source. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warn_page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Try it yourself\n",
    "\n",
    "Use the code blocks below to experiment with requesting web pages and checking out the HTML that gets returned.\n",
    "\n",
    "Some ideas to get you started:\n",
    "- `'http://ire.org'`\n",
    "- `'https://web.archive.org/web/20031202214318/http://www.tdcj.state.tx.us:80/stat/finalmeals.htm'`\n",
    "- `'https://www.nrc.gov/reactors/operating/list-power-reactor-units.html'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn your HTML into soup\n",
    "\n",
    "The HTML in the `.text` attribute of the request object is just a string -- a big ol' chunk of text.\n",
    "\n",
    "Before we start targeting and extracting pieces of data in the HTML, we need to turn that chunk of text into a data structure that Python can work with. That's where the [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (`bs4`) library comes in.\n",
    "\n",
    "We'll create a new instance of a `BeautifulSoup` object, which lives under the top-level `bs4` library that we imported earlier. We need to give it two things:\n",
    "- The HTML we'd like to parse -- `warn_page.text`\n",
    "- A string with the name of the type of parser to use -- `html.parser` is the default and usually fine, but [there are other options](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser)\n",
    "\n",
    "We'll save the parsed HTML as a new variable, `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(warn_page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing happened, which is good! You can take a look at what `soup` is, but it looks pretty much like `warn_page.text`, except a bit easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to be sure, you can use the Python function `type()` to check what sort of object you're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `str` type means a string, or text\n",
    "type(warn_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `bs4.BeautifulSoup` type means we successfully created the object\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Try it yourself\n",
    "\n",
    "Use the code blocks below to experiment fetching HTML and turning it into soup (if you fetched some pages earlier and saved them as variables, that'd be a good start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target and extract the data\n",
    "\n",
    "Now that we have BeautifulSoup object loaded up, we can go hunting for the specific HTML elements that contain the data we need. Our general strategy:\n",
    "1. Find the main table with the data we want to grab\n",
    "2. Get a list of rows (the `tr` element, which stands for \"table row\") in that table\n",
    "3. Use a Python `for loop` to go through each table row and find the data inside it (`td`, or \"table data\")\n",
    "\n",
    "To accomplish this, we'll use two `bs4` methods:\n",
    "- [`find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find), which returns the first element that matches whatever criteria you hand it\n",
    "- [`find_all()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all), which returns a _list_ of elements that match the criteria. ([Here's how Python lists work](Python%20syntax%20cheat%20sheet.ipynb#Lists).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the table\n",
    "\n",
    "To start with, we need to find the table. There are several ways to accomplish this, but because this is the only table on the page (view source and `Ctrl+F` to search for `<table` to confirm), we can simply say, \"Look through the `soup` object and find the table tag.\"\n",
    "\n",
    "Translated, the code is: `soup.find('table')`. While we're at it, save the results of that search to a new variable, `table`.\n",
    "\n",
    "Run these cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the rows in the table\n",
    "\n",
    "Next, use the `find_all()` method to drill down and get a list of rows in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how many items are in this list -- in other words, how many rows are in the table -- you can use the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through the rows and extract the data\n",
    "\n",
    "Next, we can use a [`for` loop](Python%20syntax%20cheat%20sheet.ipynb#for-loops) to go through the list of rows and start grabbing data from each one.\n",
    "\n",
    "Quick refresher on _for loop_ syntax: Start with the word `for` (lowercase), then a variable name to stand in for each item in the list that you're looping over, then the word `in` (lowercase), then the name of the list holding the items (`rows`, in our case), then a colon, then an indented block of code describing what we're doing to each item in the list.\n",
    "\n",
    "Each piece of data in the row will be stored in a `td` tag, which stands for \"table data.\" So inside the loop -- in the indented block -- we'll use the `find_all()` method to get a list of every `td` tag inside the row. And from there, we can access the content inside each tag.\n",
    "\n",
    "Our goal is to end up with a _list_ of data for each row that we will eventually write out to a file. Typically you'd probably do the work of looping and inspecting the results, step by step, in one code cell. But to show the thinking of how you might approach this (and to practice the syntax), we'll start by just printing out each row and then build from there. (`print('='*80)` will print a line of 80 equals signs -- a way to help us see exactly what we're working with in each row.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    print(row)\n",
    "    print('='*80)  # prints a divider line for easier visibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first item that prints is the header row with the column labels. You are free to keep these headers if you want, but I typically skip that row and define my own list of column names.\n",
    "\n",
    "(Another thing to consider: On better-constructed web pages, the cells in the header row will be represented by `th` (\"table header\") tags, not `td` (\"table data\") tags. The next step in our `for` loop is, \"Find all of the `td` tags in this row,\" so that would be something you would need to deal with.)\n",
    "\n",
    "We can skip the first row by using _list slicing_: adding square brackets after the name of the list with some instructions about which items in the list we want to select.\n",
    "\n",
    "Here, the syntax would be: `rows[1:]`, which means, take everything in the `rows` list starting with the item in position 1 (the second item) to the end of the list. Like many programming languages, Python starts counting at 0, so the result will leave off the first item in the list -- i.e. the item in position 0, i.e. the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    print(row)\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're cooking with gas! Now that we've isolated the rows we want, let's start pulling the data out of each row. Start by using `find_all()` to grab a list of `td` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    print(cells)\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have, for each row, a _list_ of `td` tags. Next step is to look at the table and start grabbing specific values based on their position in the list and assigning them to human-readable variable names.\n",
    "\n",
    "Quick refresher on list syntax: To access a specific item in a list, use square brackets `[]` and the index number of the item you'd like to access. For instance, to get the first cell in the row -- the date that each WARN report was issued -- use `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    warn_date = cells[0]\n",
    "    print(warn_date)\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is returning the entire `Tag` object -- we just want the contents inside it. You can access the `.text` attribute of the tag to get the text inside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    warn_date = cells[0].text    \n",
    "    print(warn_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell (`[1]`), the `.text` attribute will give you the NAICS code. In the third cell (`[2]`) you'll get the name of the business. Etc.\n",
    "\n",
    "It's also generally good practice to trim off external whitespace for each value, and you can use the Python built-in string method `strip()` to accomplish this as you march across the row.\n",
    "\n",
    "Which gets us this far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    warn_date = cells[0].text.strip()\n",
    "    naics_code = cells[1].text.strip()\n",
    "    biz = cells[2].text.strip()\n",
    "    print(warn_date, naics_code, biz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Try it yourself\n",
    "\n",
    "Now that you've gotten this far, see if you can isolate the other pieces of data in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    warn_date = cells[0].text.strip()\n",
    "    naics_code = cells[1].text.strip()\n",
    "    biz = cells[2].text.strip()\n",
    "    \n",
    "    # address\n",
    "    \n",
    "    # wia_code\n",
    "    \n",
    "    # total_employees\n",
    "    \n",
    "    # effective_date\n",
    "    \n",
    "    # type_code\n",
    "\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the results to file\n",
    "\n",
    "Now that we've targeted our lists of data for each row, we can use Python's built-in [`csv`](https://docs.python.org/3/library/csv.html) module to write each list to a CSV file.\n",
    "\n",
    "First, import the csv module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a list of headers to match the data (each column header will be a string) -- run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ['warn_date', 'naics_code', 'biz', 'address', 'wia_code',\n",
    "           'total_employees', 'effective_date', 'type_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using something called a `with` block, open a new CSV file to write to and write some code to do the following things:\n",
    "- Create a `csv.writer` object\n",
    "- Write out the list of headers using the `writerow()` method of the `csv.writer` object\n",
    "- Drop in the `for` loop you just wrote and, instead of just printing the contents of each cell, create a list of items and use the `writerow()` method of the `csv.writer` object to write your list of data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file called 'warn-data.csv' in write ('w') mode\n",
    "# specify that newlines are terminated by an empty string (this deals with a PC-specific problem)\n",
    "# and use the `as` keyword to name the open file handler (the variable name `outfile` is arbitrary)\n",
    "with open('warn-data.csv', 'w', newline='') as outfile:\n",
    "    # go to the csv module we imported and make a new .writer object attached to the open file\n",
    "    # and save it to a variable\n",
    "    writer = csv.writer(outfile)\n",
    "\n",
    "    # write out the list of headers\n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    # paste in the for loop you wrote earlier here -- watch the indentation!\n",
    "    # it should be at this indentation level =>\n",
    "    # for row in rows[1:]:\n",
    "    #     cells = row.find_all('td')\n",
    "    #     etc. ...\n",
    "    # but at the end, instead of `print(warn_date, naics_code, ...etc.)`\n",
    "    # make it something like\n",
    "    # data_out = [warn_date, naics_code, ...etc.]\n",
    "    # `writer.writerow(data_out)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the folder, you should see a new file: `warn-data.csv`. Hooray!\n",
    "\n",
    "🎉 🎉 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✍️ Try it yourself\n",
    "\n",
    "Putting it all together:\n",
    "- Find a website you'd like to scrape\n",
    "- Use `requests` to fetch the HTML\n",
    "- Use `bs4` to parse the HTML and isolate the data you're interested in\n",
    "- Use `csv` to write the data to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit problems\n",
    "\n",
    "1. **Remove internal whitespace:** Looking over the data, you probably noticed that some of the values have some unnecessary internal whitespace, which you could fix before you wrote each row to file. Python does not have a built-in string method to remove internal whitespace, unfortunately, but [Googling around](https://www.google.com/search?q=python+remove+internal+whitespace) will yield you a common strategy: Using the `split()` method to separate individual words in the string, then `join()`ing the resulting list on a single space. As an example:\n",
    "\n",
    "```python\n",
    "my_text = 'hello     world      how are      you?'\n",
    "\n",
    "# split() will turn this into a list of words\n",
    "my_text_words = my_text.split()\n",
    "# ['hello', 'world', 'how', 'are', 'you?']\n",
    "\n",
    "# join on a single space\n",
    "my_text_clean = ' '.join(my_text_words)\n",
    "print(my_text_clean)\n",
    "# prints 'hello world how are you?'\n",
    "\n",
    "# or, as a one-liner\n",
    "my_text_clean = ' '.join(my_text.split())\n",
    "```\n",
    "\n",
    "2. **Fetch multiple years:** The table we scraped has WARN notices for the current year, but the agency also maintains pages with WARN notices for previous years -- there's a list of them in a section [toward the bottom of the page](https://www.dllr.state.md.us/employment/warn.shtml). See if you can figure out how to loop over multiple pages and scrape the contents of each into a single CSV.\n",
    "\n",
    "\n",
    "3. **Build a lookup table:** Each numeric code in the \"WIA Code\" column correspondes to a local area. See if you can figure out how to create a lookup dictionary that maps the numbers to their locations, then as you're looping over the data table, replace the numeric value in that column with the name of the local area instead. Here's a hint:\n",
    "\n",
    "```python\n",
    "    lookup_dict = {\n",
    "        '1': 'hello',\n",
    "        '2': 'world'\n",
    "    }\n",
    "\n",
    "    print(lookup_dict.get('1'))\n",
    "    # prints 'hello'\n",
    "\n",
    "    print(lookup_dict.get('3'))\n",
    "    # prints None\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "4. **Fix encoding errors:** You might have noticed a few encoding problems -- e.g., `Nestlé` is being renedered as `NestlÃ©`. This is due to an encoding problem -- the `warn_page.text` is not encoded as `utf-8`. Using `decode()` and `encode()`, see if you can fix this. (Hint! It looks like the state of Maryland is a big fan of `latin-1`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
